{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "finished-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-muslim",
   "metadata": {},
   "source": [
    "### Read in Data: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "corporate-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('../Data/data_cube_subject1.mat')\n",
    "channel_labels = sio.loadmat('../Data/channel_label.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indie-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = data['data_cube']\n",
    "data_labels = data['event_label'].reshape((200,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-equilibrium",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction And Data Formatting Methods: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "centered-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Format(X:np.ndarray, \n",
    "           y:np.ndarray, \n",
    "           trials_per_condition:int=100, \n",
    "           reduction_method:str= 'Ave_over_samples'):\n",
    "    \"\"\"\n",
    "    Functionality: Method prepares data for usage in a ML algorithm. After feeding X,y through this algorithm, you can expect:\n",
    "        X and y will be split according to the different experiment conditions. Location of split is determined by trials_per_condition\n",
    "        X will have its number of dimensions reduced by one, according to the reducion method specified. Also, output data will be transposed\n",
    "            so that the first index is the number of trials.\n",
    "    Param:\n",
    "        X:: ndarray of unlabeled EEG data with dimensions (channels, samples, trials). \n",
    "        y:: ndarray of data labels for X with dimensions (trials).\n",
    "        trials_per_condition:: number of trials done for each condition. Method assumes equal number of trials per condition\n",
    "        reduction_method:: function with signature (channels x samples x trials)darray ==> ndarray, where the co-domain contains arrays of one fewer dimension than the domain\n",
    "    Returns:\n",
    "        X_by_classes_reduced:: Dictionary with keys A,B and values of dimensionality reduced X separated by condition type A or B. The first index of arrays in X_by_classes_reduced\n",
    "                                    will correspond to the trial number. The second index will correspond to a value which depends on the reduction_method.\n",
    "        y_by_classes:: Dictionary with keys A,B and values of y separated by condition type A or B.\n",
    "    \"\"\"\n",
    "    X_by_classes, y_by_classes = class_split(X,y, trials_per_condition)\n",
    "    reduce = reduction_methods[reduction_method]\n",
    "    X_by_classes_reduced = {typ: reduce(X_by_classes[typ]).T\\\n",
    "                            for typ in X_by_classes}\n",
    "    return X_by_classes_reduced, y_by_classes\n",
    "\n",
    "def class_split(X:np.ndarray, y:np.ndarray, trials_per_condition:int):\n",
    "    \"\"\"\n",
    "    Helper Method to Format, separates X and y into condition types\n",
    "    \"\"\"\n",
    "    AX, BX = X[:,:,:trials_per_condition], X[:,:,trials_per_condition:]\n",
    "    Ay, By = y[:trials_per_condition], y[trials_per_condition:]\n",
    "    X_dict, y_dict = {'A': AX, 'B':BX}, {'A':Ay, 'B':By}\n",
    "    return X_dict, y_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-shepherd",
   "metadata": {},
   "source": [
    "### Reduction Methods ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "identified-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduction methods\n",
    "#All reduction methods should assume a np.ndarray of shape (channels x samples x trials)\n",
    "#All reduction methods should return a np.ndarray of shape (dim x trials), where dim depends on the reduction method\n",
    "reduction_methods = {\n",
    "    'Ave_over_channels': lambda X: np.mean(X,0),\n",
    "    'Ave_over_samples': lambda X: np.mean(X,1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-specification",
   "metadata": {},
   "source": [
    "### Data Partitioning ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "laden-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_partition(X_dict, y_dict, test_size=0.3, random_state=None):\n",
    "    all_idx = list(range(X_dict['A'].shape[0]))\n",
    "    train_idx, test_idx = train_test_split(all_idx, test_size=test_size, random_state=random_state)\n",
    "    #make splits\n",
    "    X_dict_train = {key:X_dict[key][train_idx] for key in X_dict}\n",
    "    X_dict_test = {key:X_dict[key][test_idx] for key in X_dict}\n",
    "    y_dict_train = {key:y_dict[key][train_idx] for key in y_dict}\n",
    "    y_dict_test = {key:y_dict[key][test_idx] for key in y_dict}\n",
    "    \n",
    "    return (X_dict_train, \n",
    "            X_dict_test, \n",
    "            y_dict_train, \n",
    "            y_dict_test, \n",
    "            train_idx, \n",
    "            test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-oriental",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning and Training ###\n",
    "\n",
    "#### Hyperparameter Selection and Model Training Constants: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amazing-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To ensure the same splits across the different reduction methods, we introduce a random state\n",
    "# variable here to feed train_test_partition:\n",
    "RS = 1\n",
    "       \n",
    "#Hyperparameter dictionaries for use in grid search\n",
    "ada_params = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'learning_rate': [x/20 for x in range(1,21)],\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "    'random_state': [RS]\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': (None, 10, 50, 100, 500),\n",
    "    'min_samples_split': [2, 4, 5],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ('auto', 'sqrt', 'log2', None),\n",
    "    'min_impurity_split': (None, 0.5),\n",
    "    'random_state': [RS]\n",
    "}\n",
    "\n",
    "#Dictionary of classifiers and hyperparemeter selections\n",
    "clfs = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'RandomForestClassifier': rf_params,\n",
    "    'AdaBoostClassifier': ada_params\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-gravity",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection and Model Training Methods: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceramic-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tune_multiple_models(data_matrix:np.ndarray, \n",
    "                         data_labels:np.ndarray, \n",
    "                         trials_per_condition:int=100, \n",
    "                         clf_list:list=None, \n",
    "                         reduction_method_list:list=None):\n",
    "    if clf_list==None:\n",
    "        clf_list = [key for key in clfs]\n",
    "    if reduction_method_list==None:\n",
    "        reduction_method_list = [key for key in reduction_methods]\n",
    "    tuned_models = []\n",
    "    for clf_ident in clf_list:\n",
    "        for reduction_method_ident in reduction_method_list:\n",
    "            tuned_models.append(tune_model(data_matrix, \n",
    "                                           data_labels, \n",
    "                                           trials_per_condition,\n",
    "                                           clf_ident, \n",
    "                                           reduction_method_ident) )\n",
    "    return tuned_models\n",
    "\n",
    "def tune_model(data_matrix:np.ndarray, \n",
    "               data_labels:np.ndarray,\n",
    "               trials_per_condition:int,\n",
    "               clf_ident:str, \n",
    "               reduction_method:str):\n",
    "    #obtain classifier, hyperparameters from corresponding dictionaries\n",
    "    clf = clfs[clf_ident]\n",
    "    hyperparam_dict = params[clf_ident]\n",
    "    \n",
    "    #apply dimensionality reduction to data and split into different experiment classes\n",
    "    X_dict, y_dict = Format(data_matrix,\n",
    "                            data_labels,\n",
    "                            trials_per_condition,\n",
    "                            reduction_method)\n",
    "    #apply test_train_split\n",
    "    X_dict_train, X_dict_test, y_dict_train, y_dict_test, train_idx, test_idx = \\\n",
    "    train_test_partition(X_dict, y_dict, random_state=RS)\n",
    "    \n",
    "    #tune model\n",
    "    #For model training, train on both A and B\n",
    "    #TODO: When we have more data, it might be better to train separate models for A and B classes\n",
    "        #At the moment there is not enough data to train each class independently\n",
    "    X = np.vstack([X_dict_train['A'], X_dict_train['B']])\n",
    "    y = np.hstack([y_dict_train['A'], y_dict_train['B']])\n",
    "    clf_mod = GridSearchCV(clf, hyperparam_dict, n_jobs=7)\n",
    "    clf_mod.fit(X,y)\n",
    "    return {\n",
    "        'clf_type': clf_ident,\n",
    "        'reduction_method': reduction_method,\n",
    "        'train_idx': train_idx,\n",
    "        'test_idx': test_idx,\n",
    "        'clf': clf_mod\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "athletic-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_clfs = tune_multiple_models(data_matrix, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "partial-burst",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "downtown-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(trained_clfs, open('clfs.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eligible-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dict_ave_over_channels, y_dict_ave_over_channels = Format(data_matrix,\n",
    "                            data_labels,\n",
    "                            100,\n",
    "                            'Ave_over_channels')\n",
    "\n",
    "X_dict_ave_over_samples, y_dict_ave_over_samples = Format(data_matrix,\n",
    "                            data_labels,\n",
    "                            100,\n",
    "                            'Ave_over_samples')\n",
    "X_tr, y_tr, x_ts, y_ts, _, _ = test_train_parititon(X_dict_ave_over_channels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
