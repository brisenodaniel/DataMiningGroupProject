{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brazilian-glory",
   "metadata": {},
   "source": [
    "# Project Code Draft ##\n",
    "\n",
    "## Import Statements ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-coverage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accredited-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import inspect\n",
    "import os\n",
    "import contextlib\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-ideal",
   "metadata": {},
   "source": [
    "## Data Pre-processing Functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "foster-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function-type alias for use in type hints\n",
    "function = type(lambda x: x)\n",
    "\n",
    "######################## Data Dimensionality Reduction Methods ##############################################################\n",
    "def sliding_window(X:np.ndarray, y:np.ndarray, window_length:int, step_size:int, step:int):\n",
    "    w_start_idx = step_size*step\n",
    "    w_end_idx = min( [X.shape[1]-1, w_start_idx + window_length] )\n",
    "    if y is not None:\n",
    "        return X[:, w_start_idx:w_end_idx, :], y[w_start_idx:w_end_idx]\n",
    "    if y is None:\n",
    "        return X[:, w_start_idx:w_end_idx, :]\n",
    "\n",
    "#################### Laterized Readiness Potential Logic\n",
    "def calc_lrp(X:np.ndarray): \n",
    "    X_reduced = X[23,:,:] - X[7,:,:]\n",
    "    return X_reduced.T\n",
    "\n",
    "def sliding_lrp(X:np.ndarray, y:np.ndarray, window_length:int, step_size:int, step:int):\n",
    "    X_window, y_window = sliding_window(X, window_length, step_size, step)\n",
    "    return calc_lrp(X_window), y_window\n",
    "\n",
    "\n",
    "\n",
    "######################## MNE CSP transform logic\n",
    "\n",
    "#Constants to save last-trained CSP objects\n",
    "# and dictionary of saved CSP objects for later use\n",
    "csp_transformer = mne.decoding.CSP()\n",
    "csp_dict = {}\n",
    "\n",
    "def csp_transform(X:np.ndarray, y:np.ndarray=None, n_components:int=None, transformer_label:str=None):\n",
    "    global csp_transformer\n",
    "    global csp_dict\n",
    "    dim = X.shape\n",
    "    X_shaped = X.reshape(dim[2], dim[0], dim[1])\n",
    "    if y is not None:\n",
    "        if n_components is not None:\n",
    "            csp_transformer = mne.decoding.CSP(n_components)\n",
    "        else:\n",
    "            csp_transformer = mne.decoding.CSP()\n",
    "        csp_transformer.fit(X_shaped, y)\n",
    "        if transformer_label is not None:\n",
    "            csp_dict[transformer_label] = csp_transformer\n",
    "        return None        \n",
    "    else:\n",
    "        if transformer_label is not None:\n",
    "            return csp_dict[transformer_label].transform(X_shaped)\n",
    "        else:\n",
    "            return csp_transformer.transform(X_shaped)\n",
    "\n",
    "def sliding_csp_transform(X:np.ndarray,\n",
    "                          window_length:int, \n",
    "                          step_size:int,\n",
    "                          step:int,\n",
    "                          y:np.ndarray=None,\n",
    "                          n_components:int=None,\n",
    "                          transformer_label:str=None,\n",
    "                          refit_csp=False):\n",
    "    X_window, y_window = sliding_window(X, window_length, step_size, step)\n",
    "    if refit_csp:\n",
    "        csp_transform(X_window, y_window, n_components, transformer_label)\n",
    "    return csp_transform(X_window, transformer_label=transformer_label), y_window\n",
    "    \n",
    "def sliding_csp_fit_transform(X:np.ndarray,\n",
    "                              window_length:int,\n",
    "                              step_size:int,\n",
    "                              step:int,\n",
    "                              y:np.ndarray,\n",
    "                              n_components:int=None,\n",
    "                              transformer_label:str=None):\n",
    "    X_window, y_window = sliding_window(X, window_length, step_size, step)\n",
    "    X_tr, X_ts, y_tr, y_ts = train_test_split(X_window, y_window, train_size=0.3)\n",
    "    csp_transform(X_tr, y_tr, n_components, transfomer_label)\n",
    "    return csp_transform(X_ts), y_ts, X_tr, y_tr\n",
    "    \n",
    "######################################### Data Formatting Logic ################################################\n",
    "def Format(X:np.ndarray, \n",
    "           y:np.ndarray,  \n",
    "           reduction_method:function, \n",
    "           reduction_method_args=None,\n",
    "           trials_in_A:int=100):\n",
    "    if type(reduction_method_args) == dict:\n",
    "        X_reduced = reduction_method(X, **reduction_method_args)\n",
    "    elif type(reduction_method_args) == list:\n",
    "        X_reduced = reduction_method(X, *reduction_method_args)\n",
    "    else:\n",
    "        X_reduced = reduction_method(X)\n",
    "    condition_A_data_dict, condition_B_data_dict = condition_split(X_reduced, y, trials_in_A)\n",
    "    return condition_A_data_dict, condition_B_data_dict\n",
    "\n",
    "def condition_split(X:np.ndarray, y:np.ndarray, trials_in_A:int):\n",
    "    AX, BX = X[:trials_in_A], X[trials_in_A:]\n",
    "    Ay, By = y[:trials_in_A], y[trials_in_A:]\n",
    "    A_dict = {'X': AX, 'y': Ay}\n",
    "    B_dict = {'X': BX, 'y': By}\n",
    "    return A_dict, B_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cube.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(inspect.signature(csp_transform).parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-burner",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-destination",
   "metadata": {},
   "source": [
    "#### Read in Hyperparameter Tuning Data ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "atmospheric-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('../Data/data_cube_subject1.mat')\n",
    "channel_labels = sio.loadmat('../Data/channel_label.mat')\n",
    "data_cube = data['data_cube']\n",
    "data_labels = data['event_label'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cooperative-gregory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 66 (2.2e-16 eps * 64 dim * 4.7e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 63 (2.2e-16 eps * 64 dim * 4.5e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#Divide data into CSP tuning and Hyperparameter Tuning Data\n",
    "data_cube.shape\n",
    "X_csp = data_cube[:,:,80:120]\n",
    "y_csp = data_labels[80:120]\n",
    "X_param_csp = np.vstack([data_cube[:,:,:80], data_cube[:,:,120:]])\n",
    "y_param_csp = np.hstack([data_labels[:80],data_labels[120:]])\n",
    "\n",
    "#fit CSP to proper data and save CSP object\n",
    "csp_transform(X_csp, y_csp, transformer_label='hyperparam_tune')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-journey",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning Constants ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "powerful-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter dictionaries for use in grid search\n",
    "ada_params = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'learning_rate': [x/20 for x in range(1,21)],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': (None, 10, 50, 100, 500),\n",
    "    'min_samples_split': [2, 4, 5],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ('auto', 'sqrt', 'log2', None),\n",
    "    'min_impurity_decrease': (0.0, 0.5)\n",
    "}\n",
    "\n",
    "#Dictionary of classifiers and hyperparemeter selections\n",
    "clfs = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'RandomForestClassifier': rf_params,\n",
    "    'AdaBoostClassifier': ada_params\n",
    "}\n",
    "\n",
    "#data dimensionality reduction methods to opitmize for\n",
    "    #note that csp transform will use n_components=4 for model hyperparameter tuning\n",
    "    # n_components will be finalized in another round of cross-validation\n",
    "    # runtime to validate all combinations of n_components and model hyperparameters too long\n",
    "    # for our current computational capacity so this will have to do\n",
    "reduction_methods = {\n",
    "    'lrp': calc_lrp,\n",
    "    'csp': lambda X: csp_transform(X, transformer_label='hyperparam_tune')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-anatomy",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning Functions ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tender-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tune_multiple_models(data_matrix:np.ndarray, \n",
    "                         data_labels:np.ndarray, \n",
    "                         clf_list:list=None, \n",
    "                         reduction_method_list:list=None):\n",
    "    if clf_list==None:\n",
    "        clf_list = [key for key in clfs]\n",
    "    if reduction_method_list==None:\n",
    "        reduction_method_list = [key for key in reduction_methods]\n",
    "    tuned_models = []\n",
    "    for clf_ident in clf_list:\n",
    "        for reduction_method_ident in reduction_method_list:\n",
    "            tuned_models.append(tune_model(data_matrix, \n",
    "                                           data_labels, \n",
    "                                           clf_ident, \n",
    "                                           reduction_method_ident) )\n",
    "    return tuned_models\n",
    "\n",
    "def tune_model(data_matrix:np.ndarray, \n",
    "               data_labels:np.ndarray,\n",
    "               clf_ident:str, \n",
    "               reduction_method_ident:str):\n",
    "    #obtain classifier, hyperparameters from corresponding dictionaries\n",
    "    clf = clfs[clf_ident]\n",
    "    hyperparam_dict = params[clf_ident]\n",
    "    reduction_method = reduction_methods[reduction_method_ident]\n",
    "    #apply dimensionality reduction to data and split into different experiment classes\n",
    "    #For model hyperparameter tuning, do not separate A and B conditions\n",
    "    #TODO: When we have more data, it might be better to tune separate models for A and B classes\n",
    "        #At the moment there is not enough data to tune each class independently\n",
    "    _, data_dict = Format(data_matrix, data_labels, reduction_method, trials_in_A=0)\n",
    "    #tune model\n",
    "    X = data_dict['X']\n",
    "    y = data_dict['y']\n",
    "    clf_mod = GridSearchCV(clf, hyperparam_dict, n_jobs=7)\n",
    "    clf_mod.fit(X,y)\n",
    "    return {\n",
    "        'clf_type': clf_ident,\n",
    "        'reduction_method': reduction_method_ident,\n",
    "        'clf': clf_mod\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Tune hyperparameters. WARNING: This block takes approximately 5 hours to complete on 7 cores\n",
    "%time trained_clfs = tune_multiple_models(data_cube, data_labels)\n",
    "with open('./bin/clfs.p','wb') as clf_pickle_file:\n",
    "    pickle.dump(trained_clfs, clf_pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "absent-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./bin/clfs.p','rb') as clf_pickle_file:\n",
    "    tuned_clfs = pickle.load(clf_pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_clfs[0]['bare_clf'] = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "flush-automation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'clf_type': 'RandomForestClassifier',\n",
       "  'reduction_method': 'lsp',\n",
       "  'clf': GridSearchCV(estimator=RandomForestClassifier(criterion='entropy',\n",
       "                                                n_estimators=50),\n",
       "               n_jobs=7,\n",
       "               param_grid={'criterion': ['gini', 'entropy'],\n",
       "                           'max_depth': (None, 10, 50, 100, 500),\n",
       "                           'max_features': ('auto', 'sqrt', 'log2', None),\n",
       "                           'min_impurity_split': (None, 0.5),\n",
       "                           'min_samples_leaf': [1, 2, 4],\n",
       "                           'min_samples_split': [2, 4, 5],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'bare_clf': RandomForestClassifier()},\n",
       " {'clf_type': 'RandomForestClassifier',\n",
       "  'reduction_method': 'csp',\n",
       "  'clf': GridSearchCV(estimator=RandomForestClassifier(criterion='entropy',\n",
       "                                                n_estimators=50),\n",
       "               n_jobs=7,\n",
       "               param_grid={'criterion': ['gini', 'entropy'],\n",
       "                           'max_depth': (None, 10, 50, 100, 500),\n",
       "                           'max_features': ('auto', 'sqrt', 'log2', None),\n",
       "                           'min_impurity_split': (None, 0.5),\n",
       "                           'min_samples_leaf': [1, 2, 4],\n",
       "                           'min_samples_split': [2, 4, 5],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'bare_clf': RandomForestClassifier(),\n",
       "  'csp_n_components': 2},\n",
       " {'clf_type': 'AdaBoostClassifier',\n",
       "  'reduction_method': 'lsp',\n",
       "  'clf': GridSearchCV(estimator=AdaBoostClassifier(algorithm='SAMME', learning_rate=0.75,\n",
       "                                            n_estimators=100),\n",
       "               n_jobs=7,\n",
       "               param_grid={'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                           'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3,\n",
       "                                             0.35, 0.4, 0.45, 0.5, 0.55, 0.6,\n",
       "                                             0.65, 0.7, 0.75, 0.8, 0.85, 0.9,\n",
       "                                             0.95, 1.0],\n",
       "                           'n_estimators': [50, 100, 200, 500]})},\n",
       " {'clf_type': 'AdaBoostClassifier',\n",
       "  'reduction_method': 'csp',\n",
       "  'clf': GridSearchCV(estimator=AdaBoostClassifier(algorithm='SAMME', learning_rate=0.75,\n",
       "                                            n_estimators=100),\n",
       "               n_jobs=7,\n",
       "               param_grid={'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                           'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3,\n",
       "                                             0.35, 0.4, 0.45, 0.5, 0.55, 0.6,\n",
       "                                             0.65, 0.7, 0.75, 0.8, 0.85, 0.9,\n",
       "                                             0.95, 1.0],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'csp_n_components': 5}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-brooklyn",
   "metadata": {},
   "source": [
    "#### Tune n_components for CSP Data Reduction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "welsh-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_component_candidates = [x for x in range(1,64)]\n",
    "    \n",
    "def tune_csp_args_cv(csp_models:list, \n",
    "                  n_component_list=list(range(1,64)), \n",
    "                  X:np.ndarray=data_cube, \n",
    "                  y:np.ndarray=data_labels,\n",
    "                  n_folds=5):\n",
    "    for clf_dict in csp_models:\n",
    "        if clf_dict['reduction_method']!='csp':\n",
    "            continue\n",
    "        if 'reduction_method_args' not in clf_dict:\n",
    "            clf_dict['reduction_method_args'] = {}\n",
    "        params = clf_dict['clf'].best_params_\n",
    "        mod = clf_dict['clf'].estimator\n",
    "        mod.set_params(**params)\n",
    "        best_n = None\n",
    "        best_acc = -1\n",
    "        for n_components in n_component_list:\n",
    "            acc_list = []\n",
    "            cv = KFold(shuffle=True, n_splits=n_folds)\n",
    "            for train_idx, test_idx in cv.split(y):\n",
    "                #fit csp transform\n",
    "                 #Supress lengthy print statements from CSP\n",
    "                    #WARNING: This will supress all warnings and errors. ONLY use if code has been\n",
    "                    # tested and is known to not crash during the transform.\n",
    "                with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "                    csp_transform(X[:,:,train_idx], y[train_idx], n_components)\n",
    "                #Transform data\n",
    "                #Supress lengthy print statements from CSP\n",
    "                #WARNING: This will supress all warnings and errors ONLY use if code has been\n",
    "                    # tested and is known to not crash during the transform.\n",
    "                with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "                    _, data_dict = Format(X, y, csp_transform, trials_in_A=0)\n",
    "                X_formatted, y_formatted = data_dict['X'], data_dict['y']\n",
    "                X_tr, y_tr = X_formatted[train_idx], y_formatted[train_idx]\n",
    "                X_ts, y_ts = X_formatted[test_idx], y_formatted[test_idx]\n",
    "                mod.fit(X_tr, y_tr)\n",
    "                y_pred = mod.predict(X_ts)\n",
    "                acc_list.append(accuracy_score(y_ts, y_pred))\n",
    "            ave_acc = sum(acc_list)/len(acc_list)\n",
    "            print(\"RESULTS FOR n = \", n_components)\n",
    "            print(ave_acc)\n",
    "            if ave_acc > best_acc:\n",
    "                print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "                print(best_acc)\n",
    "                print(ave_acc)\n",
    "                print(best_n)\n",
    "                print(n_components)\n",
    "                best_acc = ave_acc\n",
    "                best_n = n_components\n",
    "        print(\"?????????????????????????????????????????????????????????????\")\n",
    "        clf_dict['reduction_method_args']['n_components'] = best_n\n",
    "    return csp_models\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accompanied-worship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR n =  1\n",
      "0.465\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "-1\n",
      "0.465\n",
      "None\n",
      "1\n",
      "RESULTS FOR n =  2\n",
      "0.515\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "0.465\n",
      "0.515\n",
      "1\n",
      "2\n",
      "RESULTS FOR n =  3\n",
      "0.49000000000000005\n",
      "RESULTS FOR n =  4\n",
      "0.54\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "0.515\n",
      "0.54\n",
      "2\n",
      "4\n",
      "RESULTS FOR n =  5\n",
      "0.45499999999999996\n",
      "?????????????????????????????????????????????????????????????\n",
      "RESULTS FOR n =  1\n",
      "0.485\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "-1\n",
      "0.485\n",
      "None\n",
      "1\n",
      "RESULTS FOR n =  2\n",
      "0.47000000000000003\n",
      "RESULTS FOR n =  3\n",
      "0.485\n",
      "RESULTS FOR n =  4\n",
      "0.45499999999999996\n",
      "RESULTS FOR n =  5\n",
      "0.49000000000000005\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "0.485\n",
      "0.49000000000000005\n",
      "1\n",
      "5\n",
      "?????????????????????????????????????????????????????????????\n",
      "CPU times: user 17min 14s, sys: 11min 3s, total: 28min 17s\n",
      "Wall time: 4min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'clf_type': 'RandomForestClassifier',\n",
       "  'reduction_method': 'lsp',\n",
       "  'clf': GridSearchCV(estimator=RandomForestClassifier(criterion='entropy',\n",
       "                                                n_estimators=50),\n",
       "               n_jobs=7,\n",
       "               param_grid={'criterion': ['gini', 'entropy'],\n",
       "                           'max_depth': (None, 10, 50, 100, 500),\n",
       "                           'max_features': ('auto', 'sqrt', 'log2', None),\n",
       "                           'min_impurity_split': (None, 0.5),\n",
       "                           'min_samples_leaf': [1, 2, 4],\n",
       "                           'min_samples_split': [2, 4, 5],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'bare_clf': RandomForestClassifier()},\n",
       " {'clf_type': 'RandomForestClassifier',\n",
       "  'reduction_method': 'csp',\n",
       "  'clf': GridSearchCV(estimator=RandomForestClassifier(criterion='entropy',\n",
       "                                                n_estimators=50),\n",
       "               n_jobs=7,\n",
       "               param_grid={'criterion': ['gini', 'entropy'],\n",
       "                           'max_depth': (None, 10, 50, 100, 500),\n",
       "                           'max_features': ('auto', 'sqrt', 'log2', None),\n",
       "                           'min_impurity_split': (None, 0.5),\n",
       "                           'min_samples_leaf': [1, 2, 4],\n",
       "                           'min_samples_split': [2, 4, 5],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'bare_clf': RandomForestClassifier(),\n",
       "  'csp_n_components': 2,\n",
       "  'reduction_method_args': {'n_components': 4}},\n",
       " {'clf_type': 'AdaBoostClassifier',\n",
       "  'reduction_method': 'lsp',\n",
       "  'clf': GridSearchCV(estimator=AdaBoostClassifier(algorithm='SAMME', learning_rate=0.75,\n",
       "                                            n_estimators=100),\n",
       "               n_jobs=7,\n",
       "               param_grid={'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                           'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3,\n",
       "                                             0.35, 0.4, 0.45, 0.5, 0.55, 0.6,\n",
       "                                             0.65, 0.7, 0.75, 0.8, 0.85, 0.9,\n",
       "                                             0.95, 1.0],\n",
       "                           'n_estimators': [50, 100, 200, 500]})},\n",
       " {'clf_type': 'AdaBoostClassifier',\n",
       "  'reduction_method': 'csp',\n",
       "  'clf': GridSearchCV(estimator=AdaBoostClassifier(algorithm='SAMME', learning_rate=0.75,\n",
       "                                            n_estimators=100),\n",
       "               n_jobs=7,\n",
       "               param_grid={'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                           'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3,\n",
       "                                             0.35, 0.4, 0.45, 0.5, 0.55, 0.6,\n",
       "                                             0.65, 0.7, 0.75, 0.8, 0.85, 0.9,\n",
       "                                             0.95, 1.0],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'csp_n_components': 5,\n",
       "  'reduction_method_args': {'n_components': 5}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#This block runs in 4 minutes on 3 cores\n",
    "tune_csp_args_cv(tuned_clfs, n_component_list=list(range(1,6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-irrigation",
   "metadata": {},
   "source": [
    "## Model Evaluation And Selection ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-execution",
   "metadata": {},
   "source": [
    "## Model Evaluation Functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_methods = {\n",
    "    'lrp': calc_lrp,\n",
    "    'csp': lambda X, n: csp_transform(X, data_labels, n)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csp_test(X_tr, y_tr, X_ts, n):\n",
    "    csp_transform(X_tr, y_tr, n)\n",
    "    return csp_transform(X_tr), csp_transform(X_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_clfs[1]\n",
    "rm_params = {'n_components':tuned_clfs[1]['csp_n_components']}\n",
    "\n",
    "evaluate_model_both_conditions(tuned_clfs[1], data_cube, data_labels, 100, csp_transform, rm_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "crazy-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(X_train:np.ndarray, \n",
    "                 X_test:np.ndarray, \n",
    "                 y_train:np.ndarray, \n",
    "                 reduction_method:function, \n",
    "                 reduction_method_args:dict):\n",
    "    #determine if reduction method requires fitting on training data\n",
    "    #the following method determines if reduction method requires data labels, if it does\n",
    "    # this implies the reduction method has to be fit to the data\n",
    "    if 'y' in list(inspect.signature(reduction_method).parameters):\n",
    "        #fit reduction method to the data\n",
    "        reduction_method(X_train, y=y_train, **reduction_method_args)\n",
    "    #do dimensionality reduction on both training and testing data\n",
    "    X_tr = reduction_method(X_train, **reduction_method_args)\n",
    "    X_ts = reduction_method(X_test, **reduction_method_args)\n",
    "    return X_tr, X_ts\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate_model_single_condition(clf_dict:dict, \n",
    "                                    X:np.ndarray, \n",
    "                                    y:np.ndarray, \n",
    "                                    reduction_method:function, \n",
    "                                    reduction_method_args:dict=None, \n",
    "                                    n_folds:int=5):\n",
    "    #array of trial accuracies\n",
    "    acc_list = []\n",
    "    #obtain reduction method arguments if not provided in parameters\n",
    "    if reduction_method_args is None:\n",
    "        if 'reduction_method_args' not in clf_dict:\n",
    "            reduction_method_args = {}\n",
    "        else:\n",
    "            reduction_method_args = clf_dict['reduction_method_args']\n",
    "    #perform k-fold cross-validation to estimate model accuracy\n",
    "    cv = KFold(shuffle=True, n_splits=n_folds)\n",
    "    for train_idx, test_idx in cv.split(y):\n",
    "        #split data into train-test partitions\n",
    "        X_tr, X_ts = X[:,:,train_idx], X[:,:,test_idx]\n",
    "        y_tr, y_ts = y[train_idx], y[test_idx]\n",
    "        #do the dimensionality-reducing transformation outlined by reduction_method\n",
    "        data = (X_tr, X_ts, y_tr)\n",
    "        X_tr, X_ts = prepare_data(*data, reduction_method, reduction_method_args)\n",
    "        #assemble and train classifier\n",
    "        params = clf_dict['clf'].best_params_\n",
    "        mod = clf_dict['clf'].estimator\n",
    "        mod.set_params(**params)\n",
    "        mod.fit(X_tr, y_tr)\n",
    "        #predict and obtain accuracy score\n",
    "        y_pred = mod.predict(X_ts)\n",
    "        acc_list.append(accuracy_score(y_ts, y_pred))\n",
    "    return np.array(acc_list)\n",
    "\n",
    "def evaluate_model_both_conditions(clf_dict:dict, \n",
    "                                   X:np.ndarray, \n",
    "                                   y:np.ndarray, \n",
    "                                   trials_in_A:int,\n",
    "                                   reduction_method:function, \n",
    "                                   reduction_method_args:dict=None, \n",
    "                                   n_folds:int=5):\n",
    "    #collect parameters which do not vary between A and B conditions\n",
    "    test_params = [reduction_method, reduction_method_args, n_folds]\n",
    "    #obtain accuracy score on A and B separately\n",
    "    A_acc = evaluate_model_single_condition(clf_dict, X[:,:,:trials_in_A], y[:trials_in_A], *test_params)\n",
    "    B_acc = evaluate_model_single_condition(clf_dict, X[:,:,trials_in_A:], y[trials_in_A:], *test_params)\n",
    "    return A_acc, B_acc\n",
    "\n",
    "def evaluate_models(clfs:list, \n",
    "                    X:np.ndarray, \n",
    "                    y:np.ndarray, \n",
    "                    trials_in_A:int, \n",
    "                    reduction_methods:dict, \n",
    "                    reduction_method_arg_list:dict=None,\n",
    "                    n_folds:int=5):\n",
    "    #assemble test conditions, test conditions consist of a tuple (classifier_dict, reduction_method, reduction_method_args)\n",
    "    if reduction_method_arg_list is None:\n",
    "        reduction_method_arg_list = { method:None for method in reduction_methods }\n",
    "    for clf in clfs:\n",
    "        reduction_method = reduction_methods[clf['reduction_method']]\n",
    "        reduction_method_args = reduction_method_arg_list[clf['reduction_method']]\n",
    "        A_acc, B_acc = evaluate_model_both_conditions(clf, X, y, trials_in_A, reduction_method, reduction_method_args, n_folds)\n",
    "        clf['A_accuracy'] = A_acc\n",
    "        clf['B_accuracy'] = B_acc\n",
    "    return clfs\n",
    "                             \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "vocal-composer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 93 (2.2e-16 eps * 64 dim * 6.5e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 64 dim * 7.3e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 94 (2.2e-16 eps * 64 dim * 6.6e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 64 dim * 7.2e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 90 (2.2e-16 eps * 64 dim * 6.3e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 64 dim * 7e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 98 (2.2e-16 eps * 64 dim * 6.9e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 64 dim * 7.3e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 87 (2.2e-16 eps * 64 dim * 6.1e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.1e+02 (2.2e-16 eps * 64 dim * 7.5e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 94 (2.2e-16 eps * 64 dim * 6.6e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 64 dim * 7e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 97 (2.2e-16 eps * 64 dim * 6.8e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 80 (2.2e-16 eps * 64 dim * 5.7e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 95 (2.2e-16 eps * 64 dim * 6.7e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 93 (2.2e-16 eps * 64 dim * 6.6e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 91 (2.2e-16 eps * 64 dim * 6.4e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 96 (2.2e-16 eps * 64 dim * 6.7e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 64 dim * 7.1e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 91 (2.2e-16 eps * 64 dim * 6.4e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 98 (2.2e-16 eps * 64 dim * 6.9e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 64 dim * 7.2e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 92 (2.2e-16 eps * 64 dim * 6.4e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 64 dim * 7e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 92 (2.2e-16 eps * 64 dim * 6.5e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.1e+02 (2.2e-16 eps * 64 dim * 7.4e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 94 (2.2e-16 eps * 64 dim * 6.6e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 64 dim * 7.2e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 90 (2.2e-16 eps * 64 dim * 6.4e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 64 dim * 7.2e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 91 (2.2e-16 eps * 64 dim * 6.4e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 87 (2.2e-16 eps * 64 dim * 6.1e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 98 (2.2e-16 eps * 64 dim * 6.9e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 96 (2.2e-16 eps * 64 dim * 6.8e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 86 (2.2e-16 eps * 64 dim * 6.1e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 99 (2.2e-16 eps * 64 dim * 7e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 93 (2.2e-16 eps * 64 dim * 6.6e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 64 dim * 7.1e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 78 (2.2e-16 eps * 64 dim * 5.5e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 64 dim * 7.2e+15  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'clf_type': 'RandomForestClassifier',\n",
       "  'reduction_method': 'lsp',\n",
       "  'clf': GridSearchCV(estimator=RandomForestClassifier(criterion='entropy',\n",
       "                                                n_estimators=50),\n",
       "               n_jobs=7,\n",
       "               param_grid={'criterion': ['gini', 'entropy'],\n",
       "                           'max_depth': (None, 10, 50, 100, 500),\n",
       "                           'max_features': ('auto', 'sqrt', 'log2', None),\n",
       "                           'min_impurity_split': (None, 0.5),\n",
       "                           'min_samples_leaf': [1, 2, 4],\n",
       "                           'min_samples_split': [2, 4, 5],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'bare_clf': RandomForestClassifier(),\n",
       "  'A_accuracy': array([0.75, 0.75, 0.75, 0.6 , 0.8 ]),\n",
       "  'B_accuracy': array([0.65, 0.8 , 0.6 , 0.6 , 0.7 ])},\n",
       " {'clf_type': 'RandomForestClassifier',\n",
       "  'reduction_method': 'csp',\n",
       "  'clf': GridSearchCV(estimator=RandomForestClassifier(criterion='entropy',\n",
       "                                                n_estimators=50),\n",
       "               n_jobs=7,\n",
       "               param_grid={'criterion': ['gini', 'entropy'],\n",
       "                           'max_depth': (None, 10, 50, 100, 500),\n",
       "                           'max_features': ('auto', 'sqrt', 'log2', None),\n",
       "                           'min_impurity_split': (None, 0.5),\n",
       "                           'min_samples_leaf': [1, 2, 4],\n",
       "                           'min_samples_split': [2, 4, 5],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'bare_clf': RandomForestClassifier(),\n",
       "  'csp_n_components': 2,\n",
       "  'reduction_method_args': {'n_components': 4},\n",
       "  'A_accuracy': array([0.5 , 0.65, 0.35, 0.35, 0.55]),\n",
       "  'B_accuracy': array([0.55, 0.6 , 0.6 , 0.45, 0.55])},\n",
       " {'clf_type': 'AdaBoostClassifier',\n",
       "  'reduction_method': 'lsp',\n",
       "  'clf': GridSearchCV(estimator=AdaBoostClassifier(algorithm='SAMME', learning_rate=0.75,\n",
       "                                            n_estimators=100),\n",
       "               n_jobs=7,\n",
       "               param_grid={'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                           'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3,\n",
       "                                             0.35, 0.4, 0.45, 0.5, 0.55, 0.6,\n",
       "                                             0.65, 0.7, 0.75, 0.8, 0.85, 0.9,\n",
       "                                             0.95, 1.0],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'A_accuracy': array([0.7 , 0.7 , 0.75, 0.7 , 0.55]),\n",
       "  'B_accuracy': array([0.55, 0.8 , 0.7 , 0.7 , 0.75])},\n",
       " {'clf_type': 'AdaBoostClassifier',\n",
       "  'reduction_method': 'csp',\n",
       "  'clf': GridSearchCV(estimator=AdaBoostClassifier(algorithm='SAMME', learning_rate=0.75,\n",
       "                                            n_estimators=100),\n",
       "               n_jobs=7,\n",
       "               param_grid={'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                           'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3,\n",
       "                                             0.35, 0.4, 0.45, 0.5, 0.55, 0.6,\n",
       "                                             0.65, 0.7, 0.75, 0.8, 0.85, 0.9,\n",
       "                                             0.95, 1.0],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'csp_n_components': 5,\n",
       "  'reduction_method_args': {'n_components': 5},\n",
       "  'A_accuracy': array([0.6 , 0.45, 0.5 , 0.6 , 0.55]),\n",
       "  'B_accuracy': array([0.55, 0.4 , 0.55, 0.55, 0.5 ])}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduction_m = {'lsp': calc_lrp, 'csp': csp_transform}\n",
    "evaluate_models(tuned_clfs, data_cube, data_labels, 100, reduction_m )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "liable-charlotte",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'clf_type': 'RandomForestClassifier',\n",
       "  'reduction_method': 'lsp',\n",
       "  'clf': GridSearchCV(estimator=RandomForestClassifier(criterion='entropy',\n",
       "                                                n_estimators=50),\n",
       "               n_jobs=7,\n",
       "               param_grid={'criterion': ['gini', 'entropy'],\n",
       "                           'max_depth': (None, 10, 50, 100, 500),\n",
       "                           'max_features': ('auto', 'sqrt', 'log2', None),\n",
       "                           'min_impurity_split': (None, 0.5),\n",
       "                           'min_samples_leaf': [1, 2, 4],\n",
       "                           'min_samples_split': [2, 4, 5],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'bare_clf': RandomForestClassifier(),\n",
       "  'A_accuracy': array([0.75, 0.75, 0.75, 0.6 , 0.8 ]),\n",
       "  'B_accuracy': array([0.65, 0.8 , 0.6 , 0.6 , 0.7 ])},\n",
       " {'clf_type': 'RandomForestClassifier',\n",
       "  'reduction_method': 'csp',\n",
       "  'clf': GridSearchCV(estimator=RandomForestClassifier(criterion='entropy',\n",
       "                                                n_estimators=50),\n",
       "               n_jobs=7,\n",
       "               param_grid={'criterion': ['gini', 'entropy'],\n",
       "                           'max_depth': (None, 10, 50, 100, 500),\n",
       "                           'max_features': ('auto', 'sqrt', 'log2', None),\n",
       "                           'min_impurity_split': (None, 0.5),\n",
       "                           'min_samples_leaf': [1, 2, 4],\n",
       "                           'min_samples_split': [2, 4, 5],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'bare_clf': RandomForestClassifier(),\n",
       "  'csp_n_components': 2,\n",
       "  'reduction_method_args': {'n_components': 4},\n",
       "  'A_accuracy': array([0.5 , 0.65, 0.35, 0.35, 0.55]),\n",
       "  'B_accuracy': array([0.55, 0.6 , 0.6 , 0.45, 0.55])},\n",
       " {'clf_type': 'AdaBoostClassifier',\n",
       "  'reduction_method': 'lsp',\n",
       "  'clf': GridSearchCV(estimator=AdaBoostClassifier(algorithm='SAMME', learning_rate=0.75,\n",
       "                                            n_estimators=100),\n",
       "               n_jobs=7,\n",
       "               param_grid={'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                           'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3,\n",
       "                                             0.35, 0.4, 0.45, 0.5, 0.55, 0.6,\n",
       "                                             0.65, 0.7, 0.75, 0.8, 0.85, 0.9,\n",
       "                                             0.95, 1.0],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'A_accuracy': array([0.7 , 0.7 , 0.75, 0.7 , 0.55]),\n",
       "  'B_accuracy': array([0.55, 0.8 , 0.7 , 0.7 , 0.75])},\n",
       " {'clf_type': 'AdaBoostClassifier',\n",
       "  'reduction_method': 'csp',\n",
       "  'clf': GridSearchCV(estimator=AdaBoostClassifier(algorithm='SAMME', learning_rate=0.75,\n",
       "                                            n_estimators=100),\n",
       "               n_jobs=7,\n",
       "               param_grid={'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                           'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3,\n",
       "                                             0.35, 0.4, 0.45, 0.5, 0.55, 0.6,\n",
       "                                             0.65, 0.7, 0.75, 0.8, 0.85, 0.9,\n",
       "                                             0.95, 1.0],\n",
       "                           'n_estimators': [50, 100, 200, 500]}),\n",
       "  'csp_n_components': 5,\n",
       "  'reduction_method_args': {'n_components': 5},\n",
       "  'A_accuracy': array([0.6 , 0.45, 0.5 , 0.6 , 0.55]),\n",
       "  'B_accuracy': array([0.55, 0.4 , 0.55, 0.55, 0.5 ])}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_clfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
